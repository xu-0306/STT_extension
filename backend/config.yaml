# Backend configuration for WhisperLiveKit server.

server:
  host: "127.0.0.1"
  port: 8765

stt:
  model: "medium" # tiny, base, small, medium, large-v3
  model_cache_dir: "" # optional; leave empty to use the default cache directory
  language: "auto" # auto, en, ja, zh
  backend_policy: "localagreement"
  backend: "auto" # auto, faster-whisper, whisper, openai-api
  min_chunk_size: 0.1
  buffer_trimming: "segment"
  buffer_trimming_sec: 15
  confidence_validation: false
  pcm_input: false # false = webm/opus via MediaRecorder
  vad: true
  vac: true
  vac_chunk_size: 0.04
  stall_timeout_sec: 15
  stall_check_interval_sec: 5
  engine_recycle_sec: 0 # rebuild STT engine periodically to release memory (0 disables)
  memory_cleanup_interval_sec: 60 # run gc/empty_cache periodically (0 disables)
  gpu_log_interval_sec: 0 # log CUDA memory stats every N seconds (0 disables)

translation:
  engine: "nllb" # nllb, ollama, openai, noop
  target_language: "zh-TW"
  cache_size: 512 # per-text translation cache size (0 disables)
  translator_cache_size: 4 # max cached translator instances
  timeout_sec: 8.0 # translation timeout; set 0 for no timeout
  debounce_ms: 300 # delay before translating final text (0 disables)
  segment_max_chars: 160 # force-translate when buffer exceeds this (0 disables)
  segment_max_ms: 4000 # force-translate after this duration (0 disables)
  partial: false
  nllb:
    model: "facebook/nllb-200-distilled-600M"
  ollama:
    model: "gemma3:4b"
    host: "http://localhost:11434"
    keep_alive: "5m"
  openai:
    model: "gpt-4o-mini"
    api_key: ""
    base_url: ""

subtitle:
  max_chars: 260
  max_sentences_default: 2
  max_sentences_cjk: 1
  max_repeat_sentences: 1
  history_lines: 2
  show_partial: true
